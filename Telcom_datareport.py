# -*- coding: utf-8 -*-
"""Moringa_Data_Science_Prep_W3_Independent_Project_2019_07_Alex_Twenji_DataReport.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18l7DNoFpqiE8g9yN0CLb33xEtzHp6i4m

In this week's independent project, you will be working as Data Scientist for MTN Cote d'Ivoire, a leading telecom company and you will be solving for the following research question.

Currently MTN Cote d'Ivoire would like to upgrade its technology infrastructure for its mobile users in Ivory Coast. Studying the given dataset, how does MTN Cote d'Ivoire go about the upgrade of its infrastructure strategy within the given cities?

**DATA PREPARATION**
"""

# importing libraries we will use

import pandas as pd
import numpy as np

# Setting ipython's maximum row and column displays
pd.set_option('display.max_columns', 50)

# Loading the datasets to our work environment
Telcom_read = pd.read_csv('Telcom_dataset.csv')
Telcom2_read = pd.read_csv('Telcom_dataset2.csv')
Telcom3_read = pd.read_csv('Telcom_dataset3.csv')
Location_read = pd.read_csv('cells_geo.csv')

# Previewing the datasets
# a) Telcom_read Dataset
Telcom_read.head()

# Observation: we'll need to clean the column headings by changing 'PRODUTC' to 'PRODUCT' 
# & 'DATETIME' to 'DATE_TIME'

# b) Telcom2_read Dataset
Telcom2_read.head()

# Observation: we'll need to clean the column headings by changing 'DW_A_NUMBER' 
# & 'DW_B_NUMBER' to 'DW_A_NUMBER_INT' & 'DW_B_NUMBER_INT' Respectively

# c) Telcom3_read Dataset
Telcom3_read.head()

# Observation: we'll need to clean the column headings by changing 'CELLID' to 'CELL_ID'
# and 'SIET_ID' to 'SITE_ID'

# d) Location_read Dataset
Location_read.head()

# notice the cleaning of data required. Suggestion is to split values along the ';' semi-colon

"""**DATA CLEANING**"""

# 1. Matching Telcom Datasets Column names as expected from the keys description table
# a) Telcom_read clean the column headings by changing 'PRODUTC' to 'PRODUCT' 
# & 'DATETIME' to 'DATE_TIME'
Telcom_read.columns = ['PRODUCT', 'VALUE', 'DATE_TIME', 'CELL_ON_SITE', 'DW_A_NUMBER_INT', 
                       'DW_B_NUMBER_INT', 'COUNTRY_A', 'COUNTRY_B', 'CELL_ID', 'SITE_ID']
Telcom_read.head()  # Previewing the table after the changes.

# b) Telcom2_read clean the column headings by changing 'DW_A_NUMBER' 
#    & 'DW_B_NUMBER' to 'DW_A_NUMBER_INT' & 'DW_B_NUMBER_INT' Respectively
Telcom2_read.columns = ['PRODUCT', 'VALUE', 'DATE_TIME', 'CELL_ON_SITE', 'DW_A_NUMBER_INT', 
                       'DW_B_NUMBER_INT', 'COUNTRY_A', 'COUNTRY_B', 'CELL_ID', 'SITE_ID']
Telcom2_read.head()

# c) Telcom3_read clean the column headings by changing 'CELLID' to 'CELL_ID'
#    and 'SIET_ID' to 'SITE_ID'
Telcom3_read.columns = ['PRODUCT', 'VALUE', 'DATE_TIME', 'CELL_ON_SITE', 'DW_A_NUMBER_INT', 
                       'DW_B_NUMBER_INT', 'COUNTRY_A', 'COUNTRY_B', 'CELL_ID', 'SITE_ID']
Telcom3_read.head()

# 2. Cleaning Location_Read Data
Location_read[';VILLES;STATUS;LOCALISATION;DECOUPZONE;ZONENAME;LONGITUDE;LATITUDE;REGION;AREA;CELL_ID;SITE_CODE']
# Observation: The columns are not separated and neither is the data

# a)Separating the rows column-wise using the semicolon ':'
Location = Location_read[';VILLES;STATUS;LOCALISATION;DECOUPZONE;ZONENAME;LONGITUDE;LATITUDE;REGION;AREA;CELL_ID;SITE_CODE'].apply(lambda x: pd.Series([i for i in x.split(';')]))
Location
# Observation: The row-wise index column sems to be replicated since the data already had it's own indexing
# We can drop the second column of indexing.
# We have to manually add the column names provided in the datasheet.

# b) Adding Column Names
Location.columns = ['INDEX','VILLES', 'STATUS', 'LOCALISATION', 'DECOUPZONE', 'ZONENAME',
                    'LONGITUDE', 'LATITUDE', 'REGION','AREA', 'CELL_ID', 'SITE_CODE']
Location.head()

# 3) Dropping unrequired columns of data i.e. CELL_ON_SITE,DW_A_NUMBER_INT,DW_B_NUMBER_INT,	
#    COUNTRY_A & COUNTRY_B from the Telcom Datasets
#    and dropping INDEX column on Location dataset

Telcom_read.head() # Invoking the Dataframe before further testing is required
Telcom = Telcom_read.drop(['CELL_ON_SITE', 'DW_A_NUMBER_INT', 'DW_B_NUMBER_INT', 'COUNTRY_A',
                  'COUNTRY_B'], axis= 1)
Telcom
# As can be seen, Telcom has data from 6th May 2012 - 7th May 2012

Telcom2_read.head()
Telcom_2 = Telcom2_read.drop(['CELL_ON_SITE', 'DW_A_NUMBER_INT', 'DW_B_NUMBER_INT', 'COUNTRY_A',
                  'COUNTRY_B'], axis= 1)
Telcom_2
# As can be seen, Telcom_2 has data from 7th May 2012 - 8th May 2012

Telcom3_read.head()
Telcom_3 = Telcom3_read.drop(['CELL_ON_SITE', 'DW_A_NUMBER_INT', 'DW_B_NUMBER_INT', 'COUNTRY_A',
                  'COUNTRY_B'], axis= 1)
Telcom_3
# As can be seen, Telcom_3 has data from 8th May 2012 - 9th May 2012
# All 3 Datasets seem to be between 23:00 hrs - 00:01 hrs

Location.head()
Geo_Location = Location.drop(['INDEX'], axis=1)
Geo_Location

# Sampling Data along the SITE_ID to check for NULL values
# Since the Telcom Datasets are similar, we'll use 1 to test this.
Telcom[Telcom['SITE_ID'].isnull()]
# Observation: Where there's no SITE_ID, There's no VALUE therefore we can assume
# there isn't infrastructure to be upgraded.

# From the above, we can make the data more compact by eliminating data with SITE_ID NULL values
Telcom1 = Telcom[Telcom['SITE_ID'].notnull()]
Telcom2 = Telcom_2[Telcom_2['SITE_ID'].notnull()]
Telcom3 = Telcom_3[Telcom_3['SITE_ID'].notnull()]
# Preview of Telcom3
Telcom3

# From the above, we got more insight, we can make our data better by getting rid of 
# all data with VALUE 0, as this represents 0 customer engagement.
Telcom1 = Telcom1[Telcom1['VALUE'] != 0]
Telcom2 = Telcom2[Telcom2['VALUE'] != 0]
Telcom3 = Telcom3[Telcom3['VALUE'] != 0]
# Preview of new Telcom3
Telcom3

# From the data below, we can see irregularities in naming conventions of """Abidjan_EST"
# & "ASSINIE"""
Geo_Location

# The problem above was rectified as shown below, including all irregularities observed
# from the data.
Geo_Location.loc[Geo_Location['DECOUPZONE'] == '"""Abidjan_EST"', 'DECOUPZONE'] = 'Abidjan_EST'
Geo_Location.loc[Geo_Location['DECOUPZONE'] == '"""Abidjan_NORD"', 'DECOUPZONE'] = 'Abidjan_NORD'
Geo_Location.loc[Geo_Location['DECOUPZONE'] == '"""Abidjan_CENTRE"', 'DECOUPZONE'] = 'Abidjan_CENTRE'
Geo_Location.loc[Geo_Location['ZONENAME'] == '"ASSINIE"""', 'ZONENAME'] = 'ASSINIE'
Geo_Location.loc[Geo_Location['ZONENAME'] == '"KRIKOREA"""', 'ZONENAME'] = 'KRIKOREA'
Geo_Location

# MERGE THE TELCOM DATASHEETS HERE!!!
Geo_Location[Geo_Location['CELL_ID'] == '1502501d78']

"""The Data Looks ready for Analysis from this point forward. The Data Preparation concluded with Data Cleaning to have appropriate Data for the analysis section

**DATA ANALYSIS**
"""

